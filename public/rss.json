{
    "version": "https://jsonfeed.org/version/1",
    "title": "David Bonan | Blog",
    "home_page_url": "https://davidbonan.io",
    "feed_url": "https://davidbonan.io/rss.json",
    "description": "David Bonan - Senior developer, entrepreneur, and general technology enthusiast",
    "icon": "https://davidbonan.io/logo.png",
    "items": [
        {
            "id": "https://davidbonan.io/blog/boosting-performance-with-web-workers",
            "content_html": "\nAs web applications become more complex, running heavy computations on the main thread can lead to unresponsive user interfaces and poor user experience. Web Workers provide a powerful solution by allowing us to run JavaScript in background threads. Let's explore how to implement them effectively.\n\n## The Single-Thread Problem\n\nJavaScript is single-threaded by nature. Consider this CPU-intensive task:\n\n```javascript\nfunction calculatePrimes(max) {\n  const primes = []\n  for (let i = 2; i <= max; i++) {\n    let isPrime = true\n    for (let j = 2; j <= Math.sqrt(i); j++) {\n      if (i % j === 0) {\n        isPrime = false\n        break\n      }\n    }\n    if (isPrime) primes.push(i)\n  }\n  return primes\n}\n\n// This will freeze the UI\nconst result = calculatePrimes(1000000)\n```\n\nRunning this directly in your application would block the main thread, freezing the UI until completion.\n\n## Enter Web Workers\n\nHere's how to move this computation to a Web Worker:\n\n```javascript\n// worker.js\nself.onmessage = function (e) {\n  const max = e.data\n  const primes = calculatePrimes(max)\n  self.postMessage(primes)\n}\n\nfunction calculatePrimes(max) {\n  // Same implementation as above\n}\n```\n\n```javascript\n// main.js\nconst worker = new Worker('worker.js')\n\nworker.onmessage = function (e) {\n  console.log('Calculated primes:', e.data)\n}\n\n// This won't block the UI\nworker.postMessage(1000000)\n```\n\n## Advanced Worker Patterns\n\n### 1. Worker Pools\n\nFor multiple parallel tasks:\n\n```typescript\nclass WorkerPool {\n  private workers: Worker[] = []\n  private queue: Function[] = []\n  private activeWorkers = 0\n\n  constructor(private maxWorkers: number) {\n    this.initialize()\n  }\n\n  private initialize() {\n    for (let i = 0; i < this.maxWorkers; i++) {\n      this.workers.push(new Worker('worker.js'))\n    }\n  }\n\n  async execute(data: any): Promise<any> {\n    return new Promise((resolve) => {\n      const worker = this.getAvailableWorker()\n      if (worker) {\n        this.runTask(worker, data, resolve)\n      } else {\n        this.queue.push(() => this.execute(data).then(resolve))\n      }\n    })\n  }\n\n  private getAvailableWorker() {\n    return this.activeWorkers < this.maxWorkers\n      ? this.workers[this.activeWorkers++]\n      : null\n  }\n\n  private runTask(worker: Worker, data: any, resolve: Function) {\n    worker.onmessage = (e) => {\n      this.activeWorkers--\n      resolve(e.data)\n      if (this.queue.length > 0) {\n        const nextTask = this.queue.shift()\n        nextTask?.()\n      }\n    }\n    worker.postMessage(data)\n  }\n}\n```\n\n### 2. Transferable Objects\n\nFor better performance with large data:\n\n```javascript\n// Create a large array buffer\nconst buffer = new ArrayBuffer(100000000)\n\n// Transfer ownership to worker (faster than copying)\nworker.postMessage(buffer, [buffer])\n\n// The buffer is no longer usable in the main thread\nconsole.log(buffer.byteLength) // 0\n```\n\n## Best Practices\n\n1. **Choose Tasks Wisely**\n\n```javascript\n// Good candidate for Web Worker\nconst heavyTask = {\n  compute: () => {\n    // CPU intensive calculations\n    // No DOM manipulation\n    // No shared state dependencies\n  },\n}\n\n// Bad candidate for Web Worker\nconst lightTask = {\n  compute: () => {\n    document.querySelector('#result').innerHTML = 'Done'\n    // DOM manipulation not allowed in workers\n  },\n}\n```\n\n2. **Error Handling**\n\n```javascript\nworker.onerror = function (error) {\n  console.error('Worker error:', error)\n  // Gracefully degrade or restart worker\n}\n\nworker.onmessageerror = function (error) {\n  console.error('Message error:', error)\n  // Handle message serialization errors\n}\n```\n\n## Performance Gains\n\nLet's measure the impact:\n\n```javascript\n// Without Worker\nconsole.time('Main Thread')\nconst result = calculatePrimes(1000000)\nconsole.timeEnd('Main Thread')\n// Main Thread: 1200ms (UI blocked)\n\n// With Worker\nconsole.time('Worker')\nconst worker = new Worker('worker.js')\nworker.onmessage = (e) => {\n  console.timeEnd('Worker')\n  // Worker: 1250ms (UI responsive)\n}\nworker.postMessage(1000000)\n```\n\n## Conclusion\n\nWeb Workers are a powerful tool for maintaining responsive applications while performing heavy computations. Key takeaways:\n\n- Use Workers for CPU-intensive tasks\n- Implement worker pools for parallel processing\n- Use transferable objects for large data\n- Handle errors gracefully\n- Measure performance gains\n\nRemember that Workers come with their own limitations and overhead. Choose them when the benefits of non-blocking execution outweigh the cost of message passing and thread management.\n",
            "url": "https://davidbonan.io/blog/boosting-performance-with-web-workers",
            "title": "Boosting Performance with Web Workers",
            "summary": "Learn how to leverage Web Workers to run CPU-intensive tasks without blocking the main thread in JavaScript applications.",
            "date_modified": "2024-11-25T00:00:00.000Z",
            "author": {
                "name": "David Bonan",
                "url": "https://davidbonan.io"
            }
        },
        {
            "id": "https://davidbonan.io/blog/building-mobile-app-with-ai-a-journey",
            "content_html": "\nAs artificial intelligence tools become increasingly sophisticated, developers are exploring new ways to leverage them in their workflow. I recently embarked on an interesting experiment: building a mobile application from scratch using AI assistance, specifically Cursor with Claude-3.5 Sonnet. Here's what I learned during this journey that led to a published app on the App Store.\n\n## Phase 1: Rapid Prototyping with AI\n\nThe initial development phase was surprisingly smooth. After a three-year break from React Native, I needed help getting back into the ecosystem.\n\n### Project Setup and Initial Development\nUsing Cursor's Composer feature, I started by providing a brief description of my desired application. The AI immediately generated a complete Expo project structure, handling all the necessary configurations and dependencies. This was particularly helpful as it eliminated the need to remember all the setup steps that might have changed during my three-year hiatus from React Native development.\n\n### Early Wins with AI\nThe initial iterations were remarkably successful. The AI-generated code compiled without errors, which was impressive considering the complexity of mobile development. Each feature request was met with coherent, working implementations. The AI showed a good understanding of React Native patterns and Expo's capabilities.\n\n### Rapid UI/UX Iteration\nPerhaps the most surprising aspect was the speed of UI/UX iteration. Instead of spending hours in design tools like Figma, I could describe changes conversationally to the AI and see them implemented immediately. This phase took only 2 hours, during which I wrote zero code manually, focusing instead on refining the user experience through rapid iteration.\n\n## Phase 2: The Reality Check\n\nAs the project grew, several challenges emerged that revealed the limitations of AI-assisted development.\n\n### Code Quality Deterioration\nThe AI's inability to effectively refactor existing code became increasingly problematic. When pivoting features or making significant changes, the AI would often add new code without properly removing or updating the old implementation. This led to:\n\n- Accumulation of dead code across multiple components\n- Increasingly complex and intertwined component structures\n- Reduced clarity in the codebase, making it harder for the AI to understand the context\n- Duplicate functionality implemented in slightly different ways\n\n### Architectural Challenges\nThe lack of proper architecture became evident as the application grew:\n\n1. **Component Bloat**: Components regularly exceeded 500 lines, mixing concerns and making maintenance difficult\n2. **State Management Chaos**: Business logic was scattered across different layers without clear organization\n3. **Poor Separation of Concerns**: Features were tightly coupled, making changes risky\n4. **Navigation Complexity**: Screen flows became convoluted with multiple navigation patterns\n5. **Performance Issues**: Unnecessary re-renders and inefficient data handling\n\n## The Cleanup Phase\n\nThe refactoring phase took significantly longer than the initial development - a full 5 hours of intensive code cleanup. This phase involved:\n\n### Code Restructuring\n- Breaking down large components into smaller, focused ones\n- Implementing proper state management patterns\n- Establishing clear boundaries between business logic and UI\n- Creating reusable components and hooks\n- Standardizing navigation patterns\n\n### Technical Debt Resolution\n- Identifying and removing unused code\n- Fixing bugs introduced during multiple iterations\n- Implementing proper error handling\n- Optimizing performance bottlenecks\n- Documenting key architectural decisions\n\n### Architecture Improvements\n- Introducing proper dependency injection\n- Implementing clean architecture principles\n- Setting up proper testing infrastructure\n- Creating clear data flow patterns\n- Establishing coding standards and best practices\n\n## Key Learnings\n\n### AI Strengths\n1. **Rapid Prototyping**: Unmatched speed in getting from concept to working prototype\n2. **Feature Exploration**: Excellent for testing different approaches quickly\n3. **Boilerplate Generation**: Efficient at creating standard patterns and structures\n4. **Documentation**: Good at generating initial documentation and comments\n5. **Learning Tool**: Helpful for understanding new frameworks or patterns\n\n### AI Limitations\n1. **Code Maintenance**: Struggles with large-scale refactoring\n2. **Architecture**: May not follow best practices for larger applications\n3. **Technical Debt**: Can create maintenance challenges if not properly managed\n4. **Context Understanding**: Difficulty maintaining consistency across multiple sessions\n5. **Performance Optimization**: May not implement the most efficient solutions\n\n## Conclusion\n\nAI tools like Cursor and Claude are incredibly powerful for prototyping and initial development, potentially replacing traditional design tools like Figma for quick proof-of-concepts. The experience resulted in a published app ([CESU Planning on the App Store](https://apps.apple.com/gb/app/cesu-planning/id6739985333?uo=2)), demonstrating that AI-assisted development is viable but requires careful management.\n\n### Recommendations for Future Projects\n1. Use AI primarily for initial prototyping and feature exploration\n2. Take control of the codebase earlier in the development cycle\n3. Implement strict architecture patterns from the start\n4. Schedule regular human-led refactoring sessions\n5. Use AI as a complementary tool rather than the primary developer\n6. Document architectural decisions early and maintain them throughout\n7. Set clear boundaries for AI involvement in different development phases\n\nThe future of AI in development is promising, but success lies in understanding both its capabilities and limitations. By leveraging AI's strengths while actively managing its limitations, developers can create successful applications more efficiently than ever before. ",
            "url": "https://davidbonan.io/blog/building-mobile-app-with-ai-a-journey",
            "title": "Building a Mobile App with AI: A Journey from Concept to App Store",
            "summary": "A real-world experience of using AI tools like Cursor and Claude to build a mobile application from scratch, highlighting both benefits and challenges.",
            "date_modified": "2025-01-31T00:00:00.000Z",
            "author": {
                "name": "David Bonan",
                "url": "https://davidbonan.io"
            }
        },
        {
            "id": "https://davidbonan.io/blog/mastering-javascript-data-structures-beyond-arrays",
            "content_html": "\nFor years, many developers, including myself, have defaulted to using Arrays for every data collection in JavaScript. We often believed that using a simple for...i loop was far more efficient than chaining methods like .filter().map().sort(), assuming it would avoid the apparent O(n) + O(n) + O(n) complexity.\n\nHowever, a simple piece of feedback about being more mindful of data structure choices completely changed my perspective. Let's explore how choosing the right data structure can transform your JavaScript code.\n\n## The Power of Proper Data Structures\n\nInstead of defaulting to Arrays, consider these powerful alternatives based on your specific needs:\n\n1. **Map** - When you need frequent lookups:\n\n```javascript\n// Instead of an array of objects\nconst users = new Map()\nusers.set('user1', { name: 'John', age: 30 })\n// O(1) lookup time\nconst user = users.get('user1')\n```\n\n2. **Set** - When you need unique values:\n\n```javascript\n// Instead of filtering arrays for unique values\nconst uniqueIds = new Set(['id1', 'id2', 'id1'])\n// Automatically handles duplicates\nconsole.log([...uniqueIds]) // ['id1', 'id2']\n```\n\n3. **WeakMap** - For ephemeral cache management:\n\n```javascript\n// Perfect for memory-sensitive caching\nconst cache = new WeakMap()\nlet object = { data: 'temporary' }\ncache.set(object, 'cached value')\n// Automatically cleaned up when object is garbage collected\n```\n\n## JavaScript Engine Optimization Tips\n\nThe JavaScript engine (V8) has some interesting optimizations that we can leverage. One crucial factor is maintaining consistent object shapes within collections. For example:\n\n```javascript\n// Faster to process - same shape\nconst consistentObjects = [{ name: 'John' }, { name: 'Jane' }]\n\n// Slower to process - different shapes\nconst inconsistentObjects = [{ name: 'John' }, { age: 30 }]\n```\n\nThe V8 engine can process the first array almost twice as fast because all objects share the same property structure.\n\n## Best Practices for Data Structure Selection\n\nWhen choosing a data structure, consider these factors:\n\n1. **Access Patterns** - How will you primarily interact with the data?\n2. **Memory Constraints** - Do you need garbage collection support?\n3. **Performance Requirements** - What operations need to be optimized?\n4. **Data Uniqueness** - Do you need to maintain unique values?\n\n## Conclusion\n\nMoving beyond the default Array can significantly improve your JavaScript applications. By choosing the right data structure for your specific use case, you can achieve better performance, cleaner code, and more maintainable applications.\n\nRemember:\n\n- Use Map for frequent lookups\n- Use Set for unique collections\n- Use WeakMap for garbage-collection-friendly caching\n- Keep object shapes consistent within collections\n\nThe next time you reach for an Array, pause and consider if another data structure might better serve your needs. Your future self (and your application's performance) will thank you.\n",
            "url": "https://davidbonan.io/blog/mastering-javascript-data-structures-beyond-arrays",
            "title": "Mastering JavaScript Data Structures: Beyond Arrays",
            "summary": "Learn how choosing the right data structure in JavaScript can significantly improve your code performance and readability.",
            "date_modified": "2024-11-15T00:00:00.000Z",
            "author": {
                "name": "David Bonan",
                "url": "https://davidbonan.io"
            }
        },
        {
            "id": "https://davidbonan.io/blog/solving-n2-complexity-with-javascript-maps",
            "content_html": "\nAs developers, we often encounter situations where we need to find relationships between elements in different arrays. The immediate solution that comes to mind is usually nested loops, leading to O(n²) complexity. \nHowever, beyond poor performance, there’s a more serious problem: in high-traffic environments, an O(n²) algorithm can block the event loop and freeze your server. \n\nLet's explore a real-world example and see how we can dramatically improve performance using JavaScript Maps.\n\n## The N² Problem\n\nConsider this common scenario where we need to match users with their orders:\n\n```javascript\nconst users = [\n  { id: 1, name: 'Alice' },\n  { id: 2, name: 'Bob' },\n  // ... potentially thousands of users\n]\n\nconst orders = [\n  { userId: 1, product: 'Laptop' },\n  { userId: 2, product: 'Phone' },\n  // ... potentially thousands of orders\n]\n\n// Inefficient O(n²) solution\nconst getUserOrders = () => {\n  return users.map((user) => {\n    const userOrders = orders.filter((order) => order.userId === user.id)\n    return {\n      ...user,\n      orders: userOrders,\n    }\n  })\n}\n```\n\nWith this approach, for each user, we're scanning through the entire orders array. If we have 1,000 users and 1,000 orders, we're performing 1,000,000 comparisons!\n\n## The Bigger Problem: Blocking the Event Loop\n\nJavaScript runs on a single thread, meaning the event loop is responsible for processing all tasks, from handling HTTP requests to database queries. When a computationally expensive operation like an O(n²) loop takes too long, it blocks the event loop, causing the server to stop responding to incoming requests. This can result in:\n- Delayed Responses: Other users experience increased latency.\n- Server Freezes: Requests can time out, degrading the user experience.\n- Wasted Resources: In a microservices architecture, a frozen service may trigger cascading failures.\n\nThis is why solving O(n²) complexity isn’t just about optimizing performance—it’s about keeping your application responsive and stable.\n\n## The Map Solution\n\nHere's how we can solve this using a Map to achieve O(n) complexity:\n\n```javascript\nconst getUserOrdersEfficient = () => {\n  // Create a Map of orders indexed by userId\n  const orderMap = new Map()\n  orders.forEach((order) => {\n    if (!orderMap.has(order.userId)) {\n      orderMap.set(order.userId, [])\n    }\n    orderMap.get(order.userId).push(order)\n  })\n\n  // Now we can look up orders directly by userId\n  return users.map((user) => ({\n    ...user,\n    orders: orderMap.get(user.id) || [],\n  }))\n}\n```\n\n## Performance Comparison\n\nLet's look at the actual performance difference:\n\n```javascript\n// Test with larger datasets\nconst generateTestData = (size) => {\n  const users = Array.from({ length: size }, (_, i) => ({\n    id: i + 1,\n    name: `User${i + 1}`,\n  }))\n  const orders = Array.from({ length: size }, (_, i) => ({\n    userId: Math.floor(Math.random() * size) + 1,\n    product: `Product${i + 1}`,\n  }))\n  return { users, orders }\n}\n\nconst { users, orders } = generateTestData(10000)\n\nconsole.time('N² Solution')\ngetUserOrders()\nconsole.timeEnd('N² Solution')\n// N² Solution: ~500ms\n\nconsole.time('Map Solution')\ngetUserOrdersEfficient()\nconsole.timeEnd('Map Solution')\n// Map Solution: ~5ms\n```\n\n## Why Maps Are Better Here\n\n1. **Single Pass Processing**: We only need to iterate through each array once\n2. **O(1) Lookups**: Map provides constant-time access to stored values\n3. **Memory Efficient**: We're trading a small amount of memory for massive performance gains\n4. **Scalability**: Performance remains linear as data size grows\n\n## Best Practices for Using Maps\n\nWhen implementing this pattern, keep in mind:\n\n1. **Pre-size Your Maps**: If you know the size of your data, you can optimize memory allocation:\n\n```javascript\nconst orderMap = new Map()\norderMap.set(user.id, [])\n```\n\n2. **Clear References**: When you're done, clear the Map to help garbage collection:\n\n```javascript\norderMap.clear()\n```\n\n## Conclusion\n\nNext time you find yourself writing nested loops, stop and consider if a Map-based solution might be more appropriate. The small effort of restructuring your data can lead to dramatic performance improvements, especially as your datasets grow.\n\nRemember:\n\n- Nested loops are often a red flag for performance issues\n- Maps provide O(1) lookup time\n- O(n²) algorithms can block the event loop and freeze the server\n- The extra memory usage is usually worth the performance gain\n- Always measure performance with realistic data sizes\n\nBy making smart choices about data structures, we can write code that not only works but scales efficiently with our applications' growth.\n",
            "url": "https://davidbonan.io/blog/solving-n2-complexity-with-javascript-maps",
            "title": "Solving N² Complexity with JavaScript Maps",
            "summary": "Learn how to transform inefficient nested loops into performant solutions using JavaScript Maps.",
            "date_modified": "2024-11-20T00:00:00.000Z",
            "author": {
                "name": "David Bonan",
                "url": "https://davidbonan.io"
            }
        },
        {
            "id": "https://davidbonan.io/blog/why-unit-tests-can-harm-your-codebase",
            "content_html": "\nAs developers, we often hear that more tests equal better code quality. However, there's a hidden trap in this thinking that can lead to inflexible codebases and make refactoring nearly impossible. Let's explore why some unit tests can actually harm your codebase and how to avoid these pitfalls.\n\n## The Problem with Over-Testing\n\nThe more you test intermediate methods—those that aren't entry points to your business logic—the more rigid your code becomes. But why is this happening?\n\n```javascript\n// Example of an intermediate method that shouldn't necessarily be tested\nconst calculateSubTotal = (items) => {\n  return items.reduce((sum, item) => sum + item.price, 0)\n}\n\n// This is the actual business logic entry point that should be tested\nconst calculateFinalPrice = (items, discountCode) => {\n  const subTotal = calculateSubTotal(items)\n  const discount = applyDiscount(subTotal, discountCode)\n  return subTotal - discount\n}\n```\n\nWhen you write tests for `calculateSubTotal`, you're essentially cementing its implementation. Any future refactoring that might want to combine or split this logic differently would require changing the tests, even if the final behavior remains the same.\n\n## When Should You Write Tests?\n\nBefore writing a test, ask yourself these crucial questions:\n\n1. **Is this method already covered by higher-level tests?**\n\n   - If your entry point method already tests this functionality, additional tests might be redundant\n\n2. **Is this method a crucial entry point?**\n\n   - Focus on testing the public API of your modules\n\n3. **Is this part of your application's contract?**\n   - Test the behaviors that other parts of your system depend on\n\n## Better Testing Strategies\n\nInstead of testing everything, focus on:\n\n```javascript\n// DO test public API endpoints\ndescribe('OrderService', () => {\n  it('should calculate final price with discount', () => {\n    const items = [{ price: 100 }, { price: 200 }]\n    const result = orderService.calculateFinalPrice(items, 'DISCOUNT10')\n    expect(result).toBe(270) // Tests the final business outcome\n  })\n})\n\n// DON'T test private implementation details\n// avoid testing calculateSubTotal() directly\n```\n\n## The Impact on Refactoring\n\nWhen you over-test, refactoring becomes challenging because:\n\n1. Every internal change requires updating multiple tests\n2. Test maintenance becomes a significant overhead\n3. Developers become reluctant to make necessary changes\n\n## Best Practices for Sustainable Testing\n\nTo maintain a healthy codebase:\n\n- Test behaviors, not implementations\n- Focus on entry points and public APIs\n- Keep internal implementations flexible\n- Write tests that support refactoring, not prevent it\n\n## Conclusion\n\nWhile testing is crucial for maintaining code quality, over-testing can be just as harmful as under-testing. By focusing on testing the right things—entry points, public APIs, and crucial business logic—you can maintain a robust test suite that verifies your application's behavior while still allowing for future refactoring and improvements.\n\nRemember: The goal of testing is to ensure your application works correctly, not to document every internal implementation detail. Make your tests work for you, not against you.\n",
            "url": "https://davidbonan.io/blog/why-unit-tests-can-harm-your-codebase",
            "title": "Why Unit Tests Can Harm Your Codebase and Prevent Refactoring",
            "summary": "Learn why over-testing intermediate methods can lead to rigid code and how to make better decisions about what to test.",
            "date_modified": "2024-11-15T00:00:00.000Z",
            "author": {
                "name": "David Bonan",
                "url": "https://davidbonan.io"
            }
        }
    ]
}