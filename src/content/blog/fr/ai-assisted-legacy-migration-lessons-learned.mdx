---
date: '2025-11-14'
language: 'fr'
title: "Migration Legacy Assistée par IA : Retour d'expérience"
category: 'Development'
image: '/images/articles/ai-migration-journey.png'
description: "Une rétrospective de l'utilisation intensive des outils IA pour une migration d'une large codebase, des échecs initiaux à un workflow qui fonctionne réellement."
timeToRead: 12
alternates:
  en: 'ai-assisted-legacy-migration-lessons-learned'
  fr: 'ai-assisted-legacy-migration-lessons-learned'
---

"Ça va être incroyable", ai-je pensé en regardant Claude Code analyser des milliers de lignes de code legacy. "Je vais juste lui donner le endpoint legacy à analyser, et l'IA va tout migrer. Cette migration qui prendrait des mois ? Terminée en quelques semaines."

Quatre heures plus tard, je fixais un écran rempli d'hallucinations qui référençait avec assurance des fonctions qui n'existaient pas, implémentait des patterns que nous n'avions jamais utilisés, et qui aurait cassé la production de manière spectaculaire.

C'était le Jour 1.

Bienvenue dans mon voyage de sept semaines pour découvrir comment utiliser l'IA pour une migration d'une large codebase legacy, pas la version formateur youtube, mais la réalité désordonnée, frustrante et finalement productive.

## Le Défi : Bienvenue dans le Monde Réel

Oubliez les articles de blog sur l'utilisation de ChatGPT pour construire une application todo. Il s'agissait d'une migration de code de production : extraire des fonctionnalités d'un monolithe legacy et les reconstruire dans un monorepo TypeScript. Pas un projet greenfield où les erreurs ne coûtent pas cher. Pas un prototype où "assez bon" suffit.

Migration réelle. Enjeux réels. Complexité réelle :

- **Systèmes interconnectés** où changer une chose en casse trois autres dont vous ne saviez pas l'existence
- **Connaissance du domaine** enterrée dans des commentaires de 2011, dispersée dans plusieurs repositories Git, vivant seulement dans la tête des développeurs seniors
- **Décisions architecturales** qui nécessitent de comprendre pourquoi quelque chose a été construit d'une certaine manière avant de pouvoir le reconstruire différemment
- **Standards de qualité** où "ça marche sur ma machine" n'est pas acceptable

C'est là que les outils IA promettent le plus de valeur - et où la plupart d'entre eux échouent spectaculairement.

## Semaine 1 : Quand l'Espoir Rencontre la Réalité

### 18 Septembre : Le Prompt Divin

Ma stratégie était simple : créer un prompt parfait contenant tout. La spécification complète de la fonctionnalité. Le contexte du code legacy. Les patterns de la nouvelle architecture. Les exigences de migration. Tout.

J'ai appuyé sur Entrée et regardé Claude Code réfléchir. Et réfléchir. Et réfléchir encore.

Dix minutes plus tard, il a livré un plan de migration complet avec des étapes d'implémentation détaillées. Un formatage magnifique. Des explications confiantes. Des hallucinations complètes.

L'IA avait inventé une couche de cache que nous n'avions pas. Référencé un pattern de gestion d'état que nous n'avions jamais utilisé. Proposé de migrer les appels base de données vers un ORM que nous n'utilisions pas. C'était impressionnant de faux.

**Heures perdues** : Trop nombreuses pour l'admettre.

**Prêt pour la production** : Négatif.

L'IA n'analysait pas notre code - elle faisait du pattern-matching avec ses données d'entraînement et espérait pour le mieux.

### 19 Septembre : Briser le Fantasme

Bien. Plan B : au lieu d'un méga-prompt, le découper. Prompt d'analyse. Prompt de planification. Prompt de revue. Prompt de spécification de tests.

Les résultats se sont immédiatement améliorés. Les hallucinations sont passées de "fiction complète" à "plutôt précis avec interprétation créative". Progrès !

Mais voilà le problème : "plutôt précis" n'est pas une chose acceptable dans le code de production. Vous ne pouvez pas déployer une fonctionnalité correcte à 85%. Ces 15% restants ? C'est là que vivent les bugs.

Je passais plus de temps à vérifier les sorties de l'IA que j'en aurais passé à lire le code moi-même.

## Semaine 2 : La Vérité Inconfortable

### 22 Septembre : Quand Je Suis Devenu le Goulot d'Étranglement

Après trois jours de lutte contre les hallucinations, le pattern est devenu clair : **à chaque fois que je comprenais réellement le code legacy avant de prompter l'IA, la sortie était dramatiquement meilleure**.

C'était presque embarrassant à quel point cela aurait dû être évident.

Quand je demandais aveuglément à l'IA d'analyser du code :

```
"Analyse ce module d'authentification et crée un plan de migration"
→ L'IA invente des patterns, fait des suppositions, confiant mais faux
```

Quand je l'analysais d'abord, puis demandais à l'IA :

```
"Ce module d'auth utilise des tokens JWT avec une stratégie de refresh personnalisée stockée dans Redis.
L'implémentation legacy a trois cas limites : [cas spécifiques].
Crée un plan de migration qui gère ces cas en utilisant notre nouveau service d'auth."
→ L'IA génère un plan précis et utilisable
```

La différence n'était pas l'IA. La différence c'était moi faisant mon travail d'abord.

**Quatre règles durement acquises ont émergé :**

1. **L'ambiguïté tue** - Chaque prompt vague produisait des hallucinations. Chaque prompt spécifique produisait du code utilisable.
2. **On ne peut pas sauter la compréhension** - L'IA amplifie votre compréhension ; elle ne la remplace pas.
3. **L'IA génère du bruit** - Les specs nécessitaient un filtrage manuel. L'IA inclurait chaque détail tangentiellement lié.
4. **La validation n'est pas négociable** - Revue humaine de chaque spec, chaque plan, chaque supposition.

Le nouveau workflow : **Analyse IA → Analyse humaine → Prompt détaillé → Génération IA → Validation humaine**

Pensez pair programming, où vous êtes le dev senior et l'IA est le junior talentueux qui a besoin de directions claires.

### L'Erreur Auto-Accept

Oh, et j'ai désactivé l'auto-accept pour les modifications IA.

Regarder l'IA modifier automatiquement des fichiers pendant que je prenais un café ? Ce n'est pas de la productivité. C'est jouer avec le code de production.

## Semaine 3 : La Facture Arrive

### 29 Septembre : Parlons du Temps

Il est temps d'affronter la musique. J'ai commencé à suivre les heures réelles :

**Par migration de fonctionnalité :**

- 25 minutes : Planification (lecture du legacy par IA, écriture des specs par IA, relecture humaine)
- 2h30 : Implémentation avec IA (révision de chaque changement)
- Inconnu : Débogage de la dérive architecturale

Ces 2h30 de "pair programming" avec l'IA ? La moitié c'était :

- Corriger le cap quand l'IA devenait folle
- Expliquer pourquoi on ne peut pas "juste tout refactoriser pour utiliser les derniers patterns"
- Annuler des changements parce que la fenêtre de contexte de l'IA s'était remplie et qu'elle avait oublié ce qu'on faisait

La vérité brutale : **l'IA prenait des décisions architecturales qu'elle n'avait aucune raison de prendre.**

**L'over-engineering est l'ennemie de la productivité**

## Octobre : Le Mois Où Tout A Cassé (Puis s'est amélioré)

### 3 Octobre : L'Épiphanie Multi-Modèle

Les spécifications initiales que j'avais créées ? Utiles pour cadrer, mais trop superficielles. L'IA continuait à faire de mauvaises suppositions parce que mes specs étaient trop approximatives.

Puis j'ai eu une idée : et si je faisais analyser le code legacy par plusieurs modèles IA indépendamment, puis fusionnais leurs insights ?

**L'expérience :**

- Claude analyse le flux d'authentification
- Codex analyse le flux d'authentification
- Cursor analyse le flux d'authentification
- Claude fusionne les trois perspectives en vérifiant le legacy et en validant les découvertes

Claude a remarqué les patterns de gestion d'erreur. Codex a attrapé les cas limites. Cursor a identifié les considérations de performance. Aucun d'eux n'a tout eu, mais ensemble ? Couverture complète.

C'était comme avoir trois devs juniors faire une revue de code - chacun attrape des problèmes différents.

**Coût en temps** : Plus élevé au départ.

**Amélioration de la qualité** : Dramatique.

**Hallucinations** : Les erreurs de chaque modèle étaient attrapées par les autres.

### 13 Octobre : Le Fond du Gouffre

J'ai décidé de tester le "développement IA autonome". Lui donner une spec claire, le laisser tourner, voir ce qui se passe.

Les résultats étaient... éducatifs.

**Ce que l'IA a livré :**

- Des types branded qui avaient l'air corrects mais avaient les mauvais types
- Des variables avec des noms suggérant un but mais des types impliquant autre chose
- Une couche d'abstraction tellement over-engineered qu'elle nécessitait des abstractions pour les abstractions

**Le pire ?** Le code compilait. Les tests passaient. Ça avait l'air professionnel.

Mais c'était faux de manière subtile et insidieuse. Le genre de faux qui glisse en production et cause des bugs un mois plus tard quand quelqu'un essaie de l'étendre.

J'ai passé plus de temps à corriger du code IA "fonctionnel" que j'en aurais passé à l'écrire correctement from scratch.

C'était mon moment le plus bas. **"Est-ce que ça fait vraiment gagner du temps ? Ou est-ce que je crée juste de la dette technique à la vitesse de l'IA ?"**

### 17 Octobre : L'Apocalypse Code Review

**Je générais du code plus vite que l'équipe ne pouvait le reviewer.**

Standup du lundi matin :

- "J'ai 3 PRs prêtes pour revue"
- "Chaque PR change 50-70 fichiers"
- "Elles sont toutes bloquées en attente de revue"

L'équipe ne pouvait pas suivre. Les PRs s'empilaient comme la neige dans une tempête.

**PRs petites fréquentes ?** Devenaient bloquantes. Tout le monde passait la journée à faire des revues de code au lieu d'écrire du code.

**PRs larges moins fréquentes ?** Illisibles. "Tu peux reviewer ces 80 changements de fichiers ?" Détestation de l'équipe assurée.

L'ironie : l'IA m'avait rendu si rapide que j'étais devenu le goulot d'étranglement de l'équipe.

On avait besoin d'une nouvelle stratégie. Branches de fonctionnalités imbriquées ? Flux de développement parallèles ? Meilleure séparation des préoccupations pour que les PRs puissent être reviewées indépendamment ?

Tout ce qui précède, probablement.

## Fin Octobre : La Révélation de l'Orchestration

### 22 Octobre : Le Salut en Cinq Étapes

Après plusieurs semaines de chaos, le pattern s'est finalement cristallisé. Chaque migration réussie suivait les mêmes cinq étapes :

1. **Définir un scope précis** (pas "migrer l'auth" mais "migrer la gestion du refresh token JWT avec fallback Redis")
2. **Le comprendre complètement** (vraiment lire le code legacy, pas juste le survoler)
3. **L'IA planifie en mode PLAN** (obtenir la stratégie avant tout code)
4. **Tout valider** (l'IA fait de bonnes suggestions et de terribles suppositions)
5. **Générer avec supervision** (auto-accept toujours désactivé, revue de chaque changement)

Simple. Évident avec le recul. Mais il a fallu un mois d'erreurs pour l'apprendre.

### 23 Octobre : La Percée Slash Command

Session de codage tard dans la nuit. Attendez, c'est quoi ça ?

**Claude Code peut déclencher des commandes slash lui même dans des sub-agents.**

Mon cerveau est immédiatement allé vers : "Et si j'orchestrais plusieurs agents IA ?"

À 2h du matin, j'avais un prototype fonctionnel :

- `/scout-explorer` → plusieurs agents analysent le codebase en parallèle via Claude, Codex et Cursor
- `/specification-plan` → synthétisent les découvertes en une spec cohérente
- `/build-prd` → génèrent un plan d'implémentation détaillé

**Premier test :**

- Temps d'analyse : 3x plus long que le prompt manuel
- Qualité : Nettement meilleure
- Complétude du PRD : Réellement complète

Je tenais quelque chose.

### 24 Octobre : Quand la Magie Cesse de Fonctionner

**Le problème :** L'orchestrateur était fragile. Ça marchait quand tout allait bien. Quand quelque chose tournait mal - non respect des consignes de l'IA, limite de tokens, prompt ambigu - ça s'effondrait.

Prompts complexes vers Codex via les outils CLI ? Peu fiable.
Coordination multi-agents multi-étapes ? Ca marche 1 fois sur 3.
Dépendre de Claude Code pour gérer l'orchestration ? Pas possible.

Le rêve de l'automation "set it and forget it" est mort ce jour-là.

J'avais besoin de quelque chose de plus stable. Quelque chose que je contrôlais. Quelque chose qui ne déciderait pas aléatoirement d'interpréter mes instructions de manière créative.

---

## Novembre : Quand Ça Click Enfin

### 2 Novembre : Construire Mon Propre Outil

Ça suffit. Plus de dépendance aux caprices de l'IA pour l'orchestration. Il est temps de construire quelque chose de fiable.

Le temps d'une soirée, j'ai construit une CLI qui orchestre tout le workflow d'analyse. Pas intelligent. Pas fancy. Juste fiable.

**Le flux :**

```
$ migration-cli analyze

→ "Quels repositories ?"  [prompt interactif]
→ "Quels modèles IA ?"    [Claude Sonnet, Claude Haiku, Codex GPT5, Cursor Composer1 - choix multiple]
→ "Description de la tâche ?" [spécifique, pas vague]
→ "Points d'entrée ?"     [où commencer l'analyse]
→ "Scope ?"              [ce qui est inclus, ce qui ne l'est pas]

[La CLI tourne en arrière-plan]
→ Clone les repos legacy concernés
→ Lance les modèles choisis en parallèle
→ Chacun analyse indépendamment
→ Fusionne les résultats en une analyse cohérente
→ Génère le PRD
→ Découpe en tâches atomiques
→ Revue parallèle de chaque tâche
→ Génère le cahier de test

Terminé.
```

**Premier run** : 15 minutes d'analyse, PRD complet, zéro hallucinations qui se sont glissées.

**Deuxième run** : Même qualité.

**Dixième run** : Toujours cohérent.

Enfin. De la répétabilité.

### 3 Novembre : La Révélation de la Fenêtre de Contexte

**La qualité de l'IA s'effondre après ~60% d'utilisation de la fenêtre de contexte.**

Ce n'est pas graduel. C'est une falaise. À 50% ? Bien. À 60% ? Hallucinations.

**La solution :**

- Stocké le contexte pertinent de la tâche en cours dans un fichier `.work-in-progress.md`
- Supprimé les serveurs MCP trop gourmands en contexte (je te regarde, Atlassian mangeant 18% de contexte juste en existant)

**Le résultat :** Les migrations pouvaient tourner aussi longtemps que nécessaire sans dégradation de qualité.

### 4 Novembre : Après l'Analyse, Le Code

La pièce manquante : des prompts standardisés.

J'ai créé trois fichiers d'instructions :

- `01_plan.md` - Comment analyser et planifier
- `02_code.md` - Comment implémenter (avec règles de style de code spécifiques)
- `03_review.md` - Comment reviewer pour les erreurs communes

Chaque tâche commence maintenant de la même manière :

1. Lire la tâche et le PRD et suivre les instructions de `01_plan.md`
2. Revue humaine du plan
3. Démarrer l'implémentation en pair programming avec l'IA via les instructions de `02_code.md`

Ajouté `.work-in-progress.md` pour suivre l'état de la tâche courante. Quand le contexte devient lourd ? Tout effacer sauf le fichier WIP. Contexte frais, pas de progrès perdu.

**La règle qui a tout changé :** Effacer le contexte IA religieusement.

Ne laissez pas les conversations grandir. Ne laissez pas les fenêtres de contexte se remplir. Recommencer fréquemment avec seulement l'information essentielle.

Ça semblait gaspilleur au début. "Mais je viens juste d'expliquer l'architecture !"

Puis j'ai réalisé : expliquer l'architecture à un contexte frais prend 2 minutes. Déboguer les hallucinations d'un contexte gonflé prend 2 heures.

Choix facile.

---

## Le Workflow Final : Ce Qui Marche Vraiment

6 novembre. Sept semaines dedans. Le workflow s'est finalement cristallisé en quelque chose de répétable et fiable.

### Phase 1 : Analyse & PRD (La Fondation)

**Lancer le CLI** pour une analyse complète :

- Plusieurs modèles IA travaillent en parallèle
- Validation croisée des découvertes
- Génération d'un PRD détaillé
- Découpage en tâches atomiques

**Puis s'arrêter et valider.**

Ne pas faire confiance aveuglément. Passer du temps à review :

- Est-ce que ce PRD correspond à la réalité ?
- Est-ce que ces tâches sont vraiment atomiques ?
- Quels cas limites l'IA a manqués ?
- Où sont les dépendances cachées ?

Corriger ce qui est faux. Approuver ce qui est juste. Copier vers la nouvelle codebase.

**Coût en temps** : 45 minutes.

**Valeur** : Évite des semaines de mauvaises suppositions.

### Phase 2 : Implémentation (La Partie Qu'On Ne Peut Pas Précipiter)

**Pour chaque tâche, suivre ces étapes religieusement :**

**1. Lire le code legacy vous-même**

Pas optionnel. Pas "l'IA va gérer ça". Vous le lisez. Vous le comprenez. Puis vous guidez l'IA.

IA sans votre compréhension = hallucinations.
IA avec votre compréhension = productivité.

**2. Générer le plan** avec `01_plan.md`

Laisser l'IA proposer l'approche. Référencer le PRD. Référencer la tâche. La laisser réfléchir à l'implémentation.

Puis review chaque supposition et orienter l'IA dès le moindre écart.

**3. Si vous démarrez une nouvelle fonctionnalité : Coder l'architecture vous-même**

C'est critique. Ne laissez pas l'IA concevoir votre architecture.

L'IA créera quelque chose qui marche en isolation mais ne s'intègre pas à votre système. Elle ajoutera des abstractions dont vous n'avez pas besoin. Elle fera des choix qui semblent bien maintenant mais font mal plus tard.

**30 minutes d'architecture manuelle économise 3 heures de refactoring.**

**4. Laisser l'IA implémenter les détails** avec `02_code.md`

Maintenant l'IA brille. L'architecture existe. Les patterns sont clairs. Laissez-la générer l'implémentation.

Mais gardez l'auto-accept OFF.

Reviewer chaque fichier. Questionner tout ce qui a l'air "intelligent". L'IA adore être intelligente. Mais vous avez besoin de code qui est ennuyeux et maintenable.

Cette partie est fastidieuse. Vous allez reviewer des douzaines de fichiers. Vous allez rejeter des changements qui marchent mais ne s'intègrent pas.

Faites-le quand même. C'est là que la déviation architecturale est attrapée.

**5. L'IA review son propre travail** avec `03_review.md`

L'IA attrape des erreurs différentes des humains. Utilisez-la.

**6. Rétrospective**

L'IA a fait la même erreur trois fois ? Mettez à jour vos prompts. Enseignez-lui vos patterns ORM. Documentez les conventions de votre équipe. Rendez les prompts plus intelligents.

**Le workflow n'est pas rapide. Mais il est cohérent.**

Chaque fonctionnalité suit le même processus. Chaque tâche reçoit le même examen. La qualité reste élevée même quand la vélocité augmente.

---

## Les Leçons Durement Acquises

Après sept semaines, des douzaines de tâches, et d'innombrables erreurs, voici ce que j'ai appris sur le développement assisté par IA à l'échelle d'une large codebase.

### Ce En Quoi L'IA Excel Vraiment

**Explorer les grandes codebases** - L'IA peut lire des milliers de lignes plus vite que n'importe quel humain. Pointez-la vers un module complexe et demandez "qu'est-ce que ça fait ?" Véritablement utile.

**Reconnaissance de patterns** - Quand correctement guidée, l'IA repère des patterns que vous manqueriez. Elle voit la même approche de gestion d'erreur répétée dans 20 fichiers pendant que vous êtes encore sur le fichier 3.

**Analyse multi-modèle** - Différents modèles attrapent différents problèmes. Comme la revue de code avec plusieurs devs, chacun apporte une perspective différente.

### Ce En Quoi L'IA Échoue Constamment

**Prendre des décisions architecturales** - L'IA va confidentement concevoir des systèmes élaborés qui marchent en isolation mais ne s'intègrent pas à votre codebase. Chaque fois que vous laissez l'IA décider de l'architecture, vous créez du travail de refactoring pour plus tard.

**Maintenir la cohérence** - Sur plusieurs fichiers, l'IA dérive. Le fichier 1 utilise un pattern. Le fichier 5 utilise un pattern différent. Le fichier 10 invente un troisième pattern. Vous devez attraper ça.

**Gérer son propre contexte** - Après 60% d'utilisation du contexte, la qualité s'effondre. L'IA ne le sait pas. Elle continuera à générer confidentement des déchets.

**Refactoriser sans casser les choses** - Demandez à l'IA d'"améliorer" le code et regardez-la créer de la dette technique à une vitesse impressionnante. Elle ajoutera des abstractions dont vous n'avez pas besoin et supprimera des patterns sur lesquels vous comptez.

### Les Quatre Règles Qui Comptent Vraiment

**1. La Gestion du Contexte Est Tout**

La qualité de l'IA a une limite dure à ~60% d'utilisation du contexte. Respectez cette limite ou payez le prix en hallucinations.

Solutions :

- Effacer le contexte fréquemment
- Démarrer de nouvelles conversations souvent
- Supprimer les serveurs MCP trop gourmands
- Utiliser des fichiers work-in-progress pour suivre l'état entre les sessions

**2. La Compréhension Humaine N'Est Pas Négociable**

Vous ne pouvez pas externaliser la compréhension à l'IA. Point.

L'IA amplifie ce que vous savez. Elle ne remplace pas le fait de savoir des choses. Chaque fois que j'ai essayé de sauter la compréhension du code legacy, je l'ai payé plus tard en temps de débogage.

**3. La Structure Bat l'Intelligence**

Le prompting ad-hoc produit des résultats incohérents. Les workflows standardisés produisent des résultats fiables.

Construisez des templates. Créez des checklists. Documentez votre processus. Rendez-le ennuyeux et répétable.

**4. Vous Êtes l'Architecte**

L'IA est un dev junior talentueux qui écrit du code propre mais prend des décisions de design questionnable.

Vous concevez l'architecture. L'IA implémente les détails. Inversez ça et vous détesterez votre codebase dans trois mois.

## Parlons du Temps

Tout le monde veut savoir : "Est-ce que l'IA fait vraiment gagner du temps ?"

La réponse n'est pas simple. Voici la vraie répartition :

| Phase                              | Investissement Temps                              |
| ---------------------------------- | ------------------------------------------------- |
| **Tentatives initiales "Full IA"** | Jours d'efforts gaspillés                         |
| **Raffinement du workflow**        | ~7 semaines d'expérimentation                     |
| **Workflow actuel par feature**    | 25 min planification + 2-3 heures implémentation  |
| **Corrections manuelles**          | Significativement réduit mais toujours nécessaire |
| **Développement d'outils (CLI)**   | Coût initial, gains de productivité continus      |

**La vérité brutale ?**

Semaine 1 : L'IA était plus lente que le travail manuel.

Semaine 2 : L'IA était aussi rapide que le travail manuel.

Semaine 4 : L'IA va 4x plus vite que le travail manuel.

**Est-ce que ça en vaut la peine ?**

Pour ce type de migration complexe : **Oui.**

**Exploration plus rapide du codebase** - L'IA lit des milliers de lignes pendant que je prends un café. C'est véritablement précieux.

**Boilerplate réduit** - Écrire le 15ème endpoint CRUD moi-même serait écrasant pour l'âme. L'IA le fait en secondes.

**Analyse multi-modèle** - Trois modèles IA analysant en parallèle coûteraient trois devs juniors. Je le fais tourner sur mon laptop.

**Découpage standardisé** - L'IA génère des listes de tâches détaillées qui me prendraient des heures.

Mais voici ce que la foule "l'IA va 10x votre productivité" ne vous dira pas :

**Vous reviewez toujours tout.** Chaque ligne. Chaque fichier. Chaque décision architecturale.

**Vous écrivez toujours l'architecture.** L'IA ne peut pas concevoir votre système. Seulement l'implémenter.

**Vous déboguez toujours les trucs bizarres.** Les bugs générés par IA sont souvent plus subtils que les bugs que vous écririez vous-même.

Le gain de productivité est réel. Mais c'est plutôt 2.5-4x pour les développeurs prudents, pas 10x.

## Si Vous Êtes Sur Le Point De Faire Ça : Lisez Ceci D'Abord

Vous planifiez une migration similaire ? Apprenez de mes erreurs.

### Les Sept Règles Pour Une Migration Assistée Par IA

**1. Ne commencez pas avec de l'IA autonome**

Je sais que c'est tentant. "Laissez juste l'IA gérer !"

Ne le faites pas. Vous perdrez des jours à corriger des hallucinations qui semblaient correctes au premier coup d'œil.

Commencez avec une supervision serrée. Relâchez graduellement en apprenant ce que l'IA réussit et où elle échoue.

**2. Investissez dans l'outillage dès le départ**

La soirée que j'ai passé à construire la CLI s'est payé en une semaine.

L'outillage custom semble être du surcoût jusqu'à ce que vous lanciez votre dixième migration de fonctionnalité et que tout marche simplement.

**3. Modèles multiples > Modèle unique**

Différents modèles IA attrapent différentes erreurs. Lancez-les en parallèle. Validez croisée les découvertes.

Oui, ça coûte plus cher. Ça attrape aussi les hallucinations avant qu'elles atteignent la production.

**4. L'auto-accept est un piège**

Regarder l'IA commiter des changements pendant que vous prenez un café semble productif.

Ça ne l'est pas. C'est jouer avec votre codebase.

Reviewez. Chaque. Changement.

**5. Vous concevez l'architecture**

30 minutes de travail d'architecture manuelle économise 3 heures de refactoring d'abstractions générées par IA qui ne s'intègrent pas à votre système.

**6. Effacez le contexte comme si la qualité de votre code en dépendait**

Parce que c'est le cas.

Après 60% d'utilisation du contexte, la qualité de l'IA s'effondre. Recommencez fréquemment à neuf.

**7. Standardisez tout**

Créez des templates de prompts. Construisez des checklists. Documentez le workflow.

La cohérence bat l'intelligence quand vous migrez des douzaines de fonctionnalités.

### La Crise de la Code Review

Avertissement : vous générerez du code plus vite que votre équipe ne peut le reviewer.

Ça crée un nouveau goulot d'étranglement. Solutions qui ont marché pour nous :

**Stratégies de branches imbriquées** - N'attendez pas l'approbation PR pour démarrer la prochaine fonctionnalité. Branchez depuis votre branche.

**PRs plus petites, atomiques** - 50 fichiers c'est trop. 5-8 c'est gérable.

**Couverture de tests lourde** - Les tests automatisés réduisent le fardeau de revue. Si les tests passent, le reviewer se concentre sur l'architecture.

**Sessions de revue programmées** - Bloquez des fenêtres d'une heure pour du travail de revue profonde. Les revues rapides de 15 minutes manquent les problèmes subtils.

### Quand Sauter Toute Cette Approche

Ce workflow a du sens pour les migrations complexes. C'est excessif pour :

**Petits projets** - Utilisez juste Cursor et avancez vite.

**Prototypes** - Qui se soucie de l'architecture ? Shippez et itérez.

**Apprendre une nouvelle techno** - Comprenez par vous-même d'abord. IA plus tard.

**Applications CRUD simples** - Patterns standards, pas de complexité, l'IA brille sans toute cette structure.

Gardez le workflow structuré pour les problèmes qui en ont vraiment besoin.

## La Suite : Le Paysage Évolutif

Sept semaines ce n'est pas long en années technologiques. Les outils s'améliorent déjà :

**Cursor** s'améliore significativement à chaque release. Ce qui peinait en septembre fonctionne sans accroc en novembre.

**Claude Code** continue d'étendre les fenêtres de contexte et d'améliorer l'orchestration. Les limites que j'ai rencontrées aujourd'hui pourraient avoir disparu le mois prochain.

**L'outillage custom** devient plus sophistiqué. La CLI que j'ai construite en un weekend a maintenant des fonctionnalités dont je ne savais pas avoir besoin.

**Composer1 et Gemini3** sont sortis entre temps et sont très prometteurs.

Voici le truc : **la meilleure approche n'est pas de choisir un outil.**

C'est de construire un workflow qui tire parti des forces de chaque outil :

- **Cursor** pour l'implémentation rapide quand vous savez exactement ce que vous voulez
- **Claude** pour l'analyse et la planification quand vous devez comprendre quelque chose de complexe
- **Codex** pour la découverte de patterns quand vous explorez du code inconnu
- **CLI custom** pour l'orchestration quand vous avez besoin de fiabilité sur la flexibilité

Les outils continueront à s'améliorer. Les principes du workflow ne changeront pas :

- Comprendre avant de générer
- Reviewer avant d'accepter
- Concevoir avant d'implémenter
- La structure bat le prompting ad-hoc

## La Conclusion

Voici ce que personne vendant des outils IA ne veut admettre :

**L'IA ne remplace pas les développeurs. Elle change sur quoi on passe notre temps.**

Avant l'IA : Je passais 50% de mon temps à lire du code legacy, 10% à réfléchir à l'architecture et 40% à écrire du code.

Avec l'IA : Je passe 10% à écrire du code, 60% à reviewer la sortie IA et prendre des décisions architecturales et 30% à lire du code legacy.

Le total d'heures ? 2-4x plus rapide qu'avant.

La différence ? Je n'écris plus de boilerplate. Je n'implémente plus le quinzième endpoint CRUD à la main. Je ne tape plus les cas de tests répétitifs.

À la place, je :

- Conçois des systèmes
- Review l'implémentation
- Attrape des bugs subtils
- Prends des décisions stratégiques

**C'est en fait... mieux ?**

Écrire du boilerplate n'a jamais été la partie fun. Concevoir des systèmes élégants ? C'est pourquoi je suis entré dans ce domaine.

L'IA gère les parties ennuyeuses. Je gère les parties intéressantes.

Après sept semaines et des douzaines de tâches migrées, cette division du travail semble finalement juste.

## En résumé

**Le développement assisté par IA à l'échelle nécessite plus de structure, pas moins.**

Vous ne pouvez pas juste pointer l'IA vers un problème et espérer pour le mieux. Vous avez besoin de :

- Supervision humaine serrée
- Gestion agressive du contexte
- Workflows standardisés
- Outillage custom pour votre cas d'usage spécifique

Construisez la structure. Apprenez les outils. Comprenez leurs limites. Reviewez tout.

Faites ça, et vous verrez de vrais gains de productivité 2-4x sur le travail complexe.

Sautez ces étapes, et vous passerez plus de temps à déboguer les hallucinations que vous n'en auriez passé à écrire le code manuellement.

**Le futur du développement n'est pas l'IA autonome.**

C'est les développeurs qui maîtrisent l'art de diriger l'IA efficacement dans des workflows bien structurés. C'est là que résident les vrais gains de productivité.

Et honnêtement ? Ce futur est déjà là. Vous devez juste être prêt à faire le travail pour y arriver.

---

_Sept semaines dedans, des douzaines de tâches migrées, et le workflow semble finalement productif. Pas magique. Pas autonome. Juste... productif. Parfois c'est suffisant._
